# Natural Language Processing

## Courses
These courses will help you understand the basics of Natural Language Processing along with enabling you to read and implement papers.
1. [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models)
This course, like all the other courses by Andrew Ng (on Coursera) is a simple overview or montage of NLP. It will prolly give you some confidence but since the assignments have a lot of readymade code, which once you see, you cannot unsee. Also, if you try reading a paper on NLP, you will not be able to do so (if all your knowledge comes from this course). So all in all, it is a simple, basic and gentle scratch on the surface.

2. [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)
This is one of the most rigorous courses on NLP and it starts from the very basics and scales up almost all the way to SOTA (since it is a relatively new course). It has a good amount of depth of concept and reasonable math. The best part is that they provide you with concepts and let you figure out how to implement them. This is very very helpful in the long run and once you are done with like 12-13 lectures of this course, you can actually read, understand and implement papers right from scratch in PyTorch.
<u>**Note:**</u> This course may seem to be boring in the beginning but the concepts (which may seem too rudimentary) taught in the lectures really go a long way. 

3. [Natural Language Processing by National Research University, Russia](https://www.coursera.org/learn/language-processing)
This course aims to teach you Natural Language Processing from the ground up, starting from dated statistical methods, and covering everything upto the latest Deep Learning based techniques. The quizzes test mathematical and theoritical knowledge, and the programming assignments make you build stuff. These result in a good blend of theory and practice, which can help if you're the kind that gets bored of just studying and not doing. The final project is enticing and will require you to deploy a telegram chatbot on an AWS machine, which gives you real world experience of how these systems run in production.

## More Courses
1. [Natural Language Processing by Andrew Ng](https://www.coursera.org/specializations/natural-language-processing?)

## Tutorials/Implementations
1. [IvLabs' Natural Language Processing Repository](https://github.com/IvLabs/Natural-Language-Processing)
2. [NLP From Scratch: Generating names with a character-level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)
3. [ Sequence-to-Sequence Modeling with nn.Transformer and TorchText](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
4. [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)
5. [PyTorch Seq-2-Seq](https://github.com/bentrevett/pytorch-seq2seq)

## Blog Posts
1. [Introduction to Word Embedding and Word2Vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
2. [Illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/)
3. [Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)
4. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

## Research Papers
Notes for some of the below-mentioned papers can be found [here](https://github.com/IvLabs/ResearchPaperNotes/tree/master/natural_language_processing).
1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
2. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
3. [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)
4. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
5. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
6. [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
